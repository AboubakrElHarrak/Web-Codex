{
    "url": "https://en.wikipedia.org/wiki/Object_recognition",
    "title": "Outline of object recognition",
    "table_of_contents": [
        "1 Approaches based on CAD-like object models",
        "1.1 Recognition by parts",
        "2 Appearance-based methods",
        "2.1 Edge matching",
        "2.2 Divide-and-Conquer search",
        "2.3 Greyscale matching",
        "2.4 Gradient matching",
        "2.5 Histograms of receptive field responses",
        "2.6 Large modelbases",
        "3 Feature-based methods",
        "3.1 Interpretation trees",
        "3.2 Hypothesize and test",
        "3.3 Pose consistency",
        "3.4 Pose clustering",
        "3.5 Invariance",
        "3.6 Geometric hashing",
        "3.7 Scale-invariant feature transform (SIFT)",
        "3.8 Speeded Up Robust Features (SURF)",
        "3.9 Bag of words representations",
        "4 Genetic algorithm",
        "5 Other approaches",
        "6 Applications",
        "7 Surveys",
        "8 See also",
        "9 Notes",
        "10 References",
        "11 External links"
    ],
    "content": [
        {
            "paragraph1": "The following outline is provided as an overview of and topical guide to object recognition:\n",
            "paragraph2": "Object recognition – technology in the field of computer vision for finding and identifying objects in an image or video sequence. Humans recognize a multitude of objects in images with little effort, despite the fact that the image of the objects may vary somewhat in different view points, in many different sizes and scales or even when they are translated or rotated. Objects can even be recognized when they are partially obstructed from view. This task is still a challenge for computer vision systems. Many approaches to the task have been implemented over multiple decades.\n"
        },
        {
            "title": "Approaches based on CAD-like object models",
            "ul1": "Edge detection\nPrimal sketch\nMarr, Mohan and Nevatia[1]\nLowe\nOlivier Faugeras\n",
            "subtitle1": "Recognition by parts",
            "ul2": "Generalized cylinders (Thomas Binford)\nGeons (Irving Biederman)\nDickinson, Forsyth and Ponce\n"
        },
        {
            "title": "Appearance-based methods",
            "ul1": "Use example images (called templates or exemplars) of the objects to perform recognition\nObjects look different under varying conditions:\nChanges in lighting or color\nChanges in viewing direction\nChanges in size/shape\nChanges in lighting or color\nChanges in viewing direction\nChanges in size/shape\nA single exemplar is unlikely to succeed reliably. However, it is impossible to represent all appearances of an object.\n",
            "subtitle1": "Edge matching",
            "ul2": "Uses edge detection techniques, such as the Canny edge detection, to find edges.\nChanges in lighting and color usually don't have much effect on image edges\nStrategy:\nDetect edges in template and image\nCompare edges images to find the template\nMust consider range of possible template positions\nDetect edges in template and image\nCompare edges images to find the template\nMust consider range of possible template positions\nMeasurements:\nGood –  count the number of overlapping edges. Not robust to changes in shape\nBetter – count the number of template edge pixels with some distance of an edge in the search image\nBest –  determine probability distribution of distance to nearest edge in search image (if template at correct position). Estimate likelihood of each template position generating image\nGood –  count the number of overlapping edges. Not robust to changes in shape\nBetter – count the number of template edge pixels with some distance of an edge in the search image\nBest –  determine probability distribution of distance to nearest edge in search image (if template at correct position). Estimate likelihood of each template position generating image\n",
            "subtitle2": "Divide-and-Conquer search",
            "ul3": "Strategy:\nConsider all positions as a set (a cell in the space of positions)\nDetermine lower bound on score at best position in cell\nIf bound is too large, prune cell\nIf bound is not too large, divide cell into subcells and try each subcell recursively\nProcess stops when cell is “small enough”\nConsider all positions as a set (a cell in the space of positions)\nDetermine lower bound on score at best position in cell\nIf bound is too large, prune cell\nIf bound is not too large, divide cell into subcells and try each subcell recursively\nProcess stops when cell is “small enough”\nUnlike multi-resolution search, this technique is guaranteed to find all matches that meet the criterion (assuming that the lower bound is accurate)\nFinding the Bound:\nTo find the lower bound on the best score, look at score for the template position represented by the center of the cell\nSubtract maximum change from the “center” position for any other position in cell (occurs at cell corners)\nTo find the lower bound on the best score, look at score for the template position represented by the center of the cell\nSubtract maximum change from the “center” position for any other position in cell (occurs at cell corners)\nComplexities arise from determining bounds on distance[citation needed]\n",
            "subtitle3": "Greyscale matching",
            "ul4": "Edges are (mostly) robust to illumination changes, however they throw away a lot of information\nMust compute pixel distance as a function of both pixel position and pixel intensity\nCan be applied to color also\n",
            "subtitle4": "Gradient matching",
            "ul5": "Another way to be robust to illumination changes without throwing away as much information is to compare image gradients\nMatching is performed like matching greyscale images\nSimple alternative: Use (normalized) correlation\n",
            "subtitle5": "Histograms of receptive field responses",
            "ul6": "Avoids explicit point correspondences\nRelations between different image points implicitly coded in the receptive field responses\nSwain and Ballard (1991),[2] Schiele and Crowley (2000),[3] Linde and Lindeberg (2004, 2012)[4][5]\n",
            "subtitle6": "Large modelbases",
            "ul7": "One approach to efficiently searching the database for a specific image to use eigenvectors of the templates (called eigenfaces)\nModelbases are a collection of geometric models of the objects that should be recognized\n"
        },
        {
            "title": "Feature-based methods",
            "ul1": "a search is used to find feasible matches between object features and image features.\nthe primary constraint is that a single position of the object must account for all of the feasible matches.\nmethods that extract features from the objects to be recognized and the images to be searched.\nsurface patches\ncorners\nlinear edges\nsurface patches\ncorners\nlinear edges\n",
            "subtitle1": "Interpretation trees",
            "ul2": "A method for searching for feasible matches, is to search through a tree.\nEach node in the tree represents a set of matches.\nRoot node represents empty set\nEach other node is the union of the matches in the parent node and one additional match.\nWildcard is used for features with no match\nRoot node represents empty set\nEach other node is the union of the matches in the parent node and one additional match.\nWildcard is used for features with no match\nNodes are “pruned” when the set of matches is infeasible.\nA pruned node has no children\nA pruned node has no children\nHistorically significant and still used, but less commonly\n",
            "subtitle2": "Hypothesize and test",
            "ul3": "General Idea:\nHypothesize a correspondence between a collection of image features and a collection of object features\nThen use this to generate a hypothesis about the projection from the object coordinate frame to the image frame\nUse this projection hypothesis to generate a rendering of the object. This step is usually known as backprojection\nCompare the rendering to the image, and, if the two are sufficiently similar, accept the hypothesis\nHypothesize a correspondence between a collection of image features and a collection of object features\nThen use this to generate a hypothesis about the projection from the object coordinate frame to the image frame\nUse this projection hypothesis to generate a rendering of the object. This step is usually known as backprojection\nCompare the rendering to the image, and, if the two are sufficiently similar, accept the hypothesis\nObtaining Hypothesis:\nThere are a variety of different ways of generating hypotheses.\nWhen camera intrinsic parameters are known, the hypothesis is equivalent to a hypothetical position and orientation – pose – for the object.\nUtilize geometric constraints\nConstruct a correspondence for small sets of object features to every correctly sized subset of image points. (These are the hypotheses)\nThere are a variety of different ways of generating hypotheses.\nWhen camera intrinsic parameters are known, the hypothesis is equivalent to a hypothetical position and orientation – pose – for the object.\nUtilize geometric constraints\nConstruct a correspondence for small sets of object features to every correctly sized subset of image points. (These are the hypotheses)\nThree basic approaches:\nObtaining Hypotheses by Pose Consistency\nObtaining Hypotheses by Pose Clustering\nObtaining Hypotheses by Using Invariants\nObtaining Hypotheses by Pose Consistency\nObtaining Hypotheses by Pose Clustering\nObtaining Hypotheses by Using Invariants\nExpense search that is also redundant, but can be improved using Randomization and/or Grouping\nRandomization\nExamining small sets of image features until likelihood of missing object becomes small\nFor each set of image features, all possible matching sets of model features must be considered.\nFormula:\n( 1 – Wc)k = Z\nW =  the fraction of image points that are “good” (w ~ m/n)\nc =  the number of correspondences necessary\nk =  the number of trials\nZ =  the probability of every trial using one (or more) incorrect correspondences\nGrouping\nIf we can determine groups of points that are likely to come from the same object, we can reduce the number of hypotheses that need to be examined\nRandomization\nExamining small sets of image features until likelihood of missing object becomes small\nFor each set of image features, all possible matching sets of model features must be considered.\nFormula:\n( 1 – Wc)k = Z\nW =  the fraction of image points that are “good” (w ~ m/n)\nc =  the number of correspondences necessary\nk =  the number of trials\nZ =  the probability of every trial using one (or more) incorrect correspondences\nExamining small sets of image features until likelihood of missing object becomes small\nFor each set of image features, all possible matching sets of model features must be considered.\nFormula:\n( 1 – Wc)k = Z\nW =  the fraction of image points that are “good” (w ~ m/n)\nc =  the number of correspondences necessary\nk =  the number of trials\nZ =  the probability of every trial using one (or more) incorrect correspondences\nW =  the fraction of image points that are “good” (w ~ m/n)\nc =  the number of correspondences necessary\nk =  the number of trials\nZ =  the probability of every trial using one (or more) incorrect correspondences\nGrouping\nIf we can determine groups of points that are likely to come from the same object, we can reduce the number of hypotheses that need to be examined\nIf we can determine groups of points that are likely to come from the same object, we can reduce the number of hypotheses that need to be examined\n",
            "subtitle3": "Pose consistency",
            "ul4": "Also called Alignment, since the object is being aligned to the image\nCorrespondences between image features and model features are not independent – Geometric constraints\nA small number of correspondences yields the object position – the others must be consistent with this\nGeneral Idea:\nIf we hypothesize a match between a sufficiently large group of image features and a sufficiently large group of object features, then we can recover the missing camera parameters from this hypothesis (and so render the rest of the object)\nIf we hypothesize a match between a sufficiently large group of image features and a sufficiently large group of object features, then we can recover the missing camera parameters from this hypothesis (and so render the rest of the object)\nStrategy:\nGenerate hypotheses using small number of correspondences (e.g. triples of points for 3D recognition)\nProject other model features into image (backproject) and verify additional correspondences\nGenerate hypotheses using small number of correspondences (e.g. triples of points for 3D recognition)\nProject other model features into image (backproject) and verify additional correspondences\nUse the smallest number of correspondences necessary to achieve discrete object poses\n",
            "subtitle4": "Pose clustering",
            "ul5": "General Idea:\nEach object leads to many correct sets of correspondences, each of which has (roughly) the same pose\nVote on pose. Use an accumulator array that represents pose space for each object\nThis is essentially a Hough transform\nEach object leads to many correct sets of correspondences, each of which has (roughly) the same pose\nVote on pose. Use an accumulator array that represents pose space for each object\nThis is essentially a Hough transform\nStrategy:\nFor each object, set up an accumulator array that represents pose space – each element in the accumulator array corresponds to a “bucket” in pose space.\nThen take each image frame group, and hypothesize a correspondence between it and every frame group on every object\nFor each of these correspondences, determine pose parameters and make an entry in the accumulator array for the current object at the pose value.\nIf there are large numbers of votes in any object's accumulator array, this can be interpreted as evidence for the presence of that object at that pose.\nThe evidence can be checked using a verification method\nFor each object, set up an accumulator array that represents pose space – each element in the accumulator array corresponds to a “bucket” in pose space.\nThen take each image frame group, and hypothesize a correspondence between it and every frame group on every object\nFor each of these correspondences, determine pose parameters and make an entry in the accumulator array for the current object at the pose value.\nIf there are large numbers of votes in any object's accumulator array, this can be interpreted as evidence for the presence of that object at that pose.\nThe evidence can be checked using a verification method\nNote that this method uses sets of correspondences, rather than individual correspondences\nImplementation is easier, since each set yields a small number of possible object poses.\nImplementation is easier, since each set yields a small number of possible object poses.\nImprovement\nThe noise resistance of this method can be improved by not counting votes for objects at poses where the vote is obviously unreliable\n§ For example, in cases where, if the object was at that pose, the object frame group would be invisible.\nThese improvements are sufficient to yield working systems\nThe noise resistance of this method can be improved by not counting votes for objects at poses where the vote is obviously unreliable\nThese improvements are sufficient to yield working systems\n",
            "subtitle5": "Invariance",
            "ul6": "There are geometric properties that are invariant to camera transformations\nMost easily developed for images of planar objects, but can be applied to other cases as well\n",
            "subtitle6": "Geometric hashing",
            "ul7": "An algorithm that uses geometric invariants to vote for object hypotheses\nSimilar to pose clustering, however instead of voting on pose, we are now voting on geometry\nA technique originally developed for matching geometric features (uncalibrated affine views of plane models) against a database of such features\nWidely used for pattern-matching, CAD/CAM, and medical imaging.\nIt is difficult to choose the size of the buckets\nIt is hard to be sure what “enough” means. Therefore, there may be some danger that the table will get clogged.\n",
            "subtitle7": "Scale-invariant feature transform (SIFT)",
            "ul8": "Keypoints of objects are first extracted from a set of reference images and stored in a database\nAn object is recognized in a new image by individually comparing each feature from the new image to this database and finding candidate matching features based on Euclidean distance of their feature vectors.\nLowe (2004)[6][7]\n",
            "subtitle8": "Speeded Up Robust Features (SURF)",
            "ul9": "A robust image detector & descriptor\nThe standard version is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT\nBased on sums of approximated 2D Haar wavelet responses and made efficient use of integral images.\nBay et al. (2008)[8]\n",
            "subtitle9": "Bag of words representations"
        },
        {
            "title": "Genetic algorithm",
            "paragraph1": "Genetic algorithms can operate without prior knowledge of a given dataset and can develop recognition procedures without human intervention. A recent project achieved 100 percent accuracy on the benchmark motorbike, face, airplane and car image datasets from Caltech and 99.4 percent accuracy on fish species image datasets.\n"
        },
        {
            "title": "Other approaches",
            "ul1": "3D object recognition and reconstruction[11]\nBiologically inspired object recognition\nArtificial neural networks and Deep Learning especially convolutional neural networks\nContext[12][13]\nExplicit and implicit 3D object models\nFast indexing[14]\nGlobal scene representations[12]\nGradient histograms\nStochastic grammars[15]\nIntraclass transfer learning\nObject categorization from image search\nReflectance[16]\nShape-from-shading[17]\nTemplate matching\nTexture[18]\nTopic models[13]\nUnsupervised learning\nWindow-based detection\nDeformable Part Model\nBingham distribution[19]\n"
        },
        {
            "title": "Applications",
            "paragraph1": "Object recognition methods has the following applications:\n",
            "ul1": "Activity recognition[20]\nAutomatic image annotation[21][22]\nAutomatic target recognition\nAndroid Eyes - Object Recognition[23]\nComputer-aided diagnosis[24]\nImage panoramas[25]\nImage watermarking[26]\nGlobal robot localization[27]\nFace detection[28]\nOptical Character Recognition[29]\nManufacturing quality control[30]\nContent-based image retrieval[31]\nObject Counting and Monitoring[32]\nAutomated parking systems[33]\nVisual Positioning and tracking[34]\nVideo stabilization[35]\nPedestrian detection\n"
        },
        {
            "title": "Surveys",
            "ul1": "Daniilides and Eklundh, Edelman.\nRoth, Peter M. & Winter, Martin (2008). \"SURVEYOFAPPEARANCE-BASED METHODS FOR OBJECT RECOGNITION\" (PDF). Technical Report. ICG-TR-01/08.\n"
        }
    ],
    "links": [
        "https://en.wikipedia.org/wiki/Cognitive_neuroscience_of_visual_object_recognition",
        "https://en.wikipedia.org/wiki/Computer_vision",
        "https://en.wikipedia.org/wiki/Edge_detection",
        "https://en.wikipedia.org/wiki/Primal_sketch",
        "https://en.wikipedia.org/wiki/Olivier_Faugeras",
        "https://en.wikipedia.org/wiki/Thomas_Binford",
        "https://en.wikipedia.org/wiki/Irving_Biederman",
        "https://en.wikipedia.org/wiki/Canny_edge_detector",
        "https://en.wikipedia.org/wiki/Eigenface",
        "https://en.wikipedia.org/wiki/Edge_detection",
        "https://en.wikipedia.org/wiki/Canny_edge_detector",
        "https://en.wikipedia.org/wiki/Deriche_edge_detector",
        "https://en.wikipedia.org/wiki/Sobel_operator",
        "https://en.wikipedia.org/wiki/Prewitt_operator",
        "https://en.wikipedia.org/wiki/Roberts_cross",
        "https://en.wikipedia.org/wiki/Corner_detection",
        "https://en.wikipedia.org/wiki/Blob_detection",
        "https://en.wikipedia.org/wiki/Difference_of_Gaussians",
        "https://en.wikipedia.org/wiki/Maximally_stable_extremal_regions",
        "https://en.wikipedia.org/wiki/Ridge_detection",
        "https://en.wikipedia.org/wiki/Hough_transform",
        "https://en.wikipedia.org/wiki/Generalized_Hough_transform",
        "https://en.wikipedia.org/wiki/Structure_tensor",
        "https://en.wikipedia.org/wiki/Generalized_structure_tensor",
        "https://en.wikipedia.org/wiki/Affine_shape_adaptation",
        "https://en.wikipedia.org/wiki/Harris_affine_region_detector",
        "https://en.wikipedia.org/wiki/Hessian_affine_region_detector",
        "https://en.wikipedia.org/wiki/Speeded_up_robust_features",
        "https://en.wikipedia.org/wiki/GLOH",
        "https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients",
        "https://en.wikipedia.org/wiki/Scale_space",
        "https://en.wikipedia.org/wiki/Scale_space_implementation",
        "https://en.wikipedia.org/wiki/Image_feature",
        "https://en.wikipedia.org/wiki/Feature_extraction",
        "https://en.wikipedia.org/wiki/Correspondence_problem",
        "https://en.wikipedia.org/wiki/Backprojection",
        "https://en.wikipedia.org/wiki/Pose_clustering",
        "https://en.wikipedia.org/wiki/Hough_transform",
        "https://en.wikipedia.org/wiki/Geometric_hashing",
        "https://en.wikipedia.org/wiki/Speeded_Up_Robust_Features",
        "https://en.wikipedia.org/wiki/Bag_of_words_model_in_computer_vision",
        "https://en.wikipedia.org/wiki/Genetic_algorithm",
        "https://en.wikipedia.org/wiki/3D_object_recognition",
        "https://en.wikipedia.org/wiki/3D_reconstruction",
        "https://en.wikipedia.org/wiki/Biologically_inspired",
        "https://en.wikipedia.org/wiki/Artificial_neural_networks",
        "https://en.wikipedia.org/wiki/Deep_Learning",
        "https://en.wikipedia.org/wiki/Convolutional_neural_network",
        "https://en.wikipedia.org/wiki/Context_awareness",
        "https://en.wikipedia.org/wiki/Implicit_surface",
        "https://en.wikipedia.org/wiki/Stochastic_grammar",
        "https://en.wikipedia.org/wiki/Transfer_learning",
        "https://en.wikipedia.org/wiki/Object_categorization_from_image_search",
        "https://en.wikipedia.org/wiki/Reflectance",
        "https://en.wikipedia.org/wiki/Shape_from_Shading",
        "https://en.wikipedia.org/wiki/Template_matching",
        "https://en.wikipedia.org/wiki/Topic_model",
        "https://en.wikipedia.org/wiki/Unsupervised_learning",
        "https://en.wikipedia.org/wiki/Bingham_distribution",
        "https://en.wikipedia.org/wiki/Activity_recognition",
        "https://en.wikipedia.org/wiki/Automatic_image_annotation",
        "https://en.wikipedia.org/wiki/Automatic_target_recognition",
        "https://en.wikipedia.org/wiki/Panorama",
        "https://en.wikipedia.org/wiki/Image_watermarking",
        "https://en.wikipedia.org/wiki/Robot_localization",
        "https://en.wikipedia.org/wiki/Face_detection",
        "https://en.wikipedia.org/wiki/Optical_Character_Recognition",
        "https://en.wikipedia.org/wiki/Quality_control",
        "https://en.wikipedia.org/wiki/Automated_parking_system",
        "https://en.wikipedia.org/wiki/Video_tracking",
        "https://en.wikipedia.org/wiki/Video_stabilization",
        "https://en.wikipedia.org/wiki/Pedestrian_detection",
        "https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients",
        "https://en.wikipedia.org/wiki/Convolutional_neural_network",
        "https://en.wikipedia.org/wiki/OpenCV",
        "https://en.wikipedia.org/wiki/Object_detection",
        "https://en.wikipedia.org/wiki/Speeded_up_robust_features",
        "https://en.wikipedia.org/wiki/Template_matching",
        "https://en.wikipedia.org/wiki/Integral_channel_feature",
        "https://en.wikipedia.org/wiki/List_of_computer_vision_topics",
        "https://en.wikipedia.org/wiki/List_of_emerging_technologies",
        "https://en.wikipedia.org/wiki/Outline_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Google_Books",
        "https://en.wikipedia.org/wiki/Wayback_Machine",
        "https://en.wikipedia.org/wiki/Outline_of_object_recognition",
        "https://en.wikipedia.org/wiki/Outline_of_object_recognition",
        "https://en.wikipedia.org/wiki/Main_Page",
        "https://en.wikipedia.org/wiki/Main_Page"
    ]
}