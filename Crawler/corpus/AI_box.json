{
    "url": "https://en.wikipedia.org/wiki/AI_box",
    "title": "AI box",
    "table_of_contents": [
        "1 Motivation",
        "2 Avenues of escape",
        "2.1 Physical",
        "2.2 Social engineering",
        "2.2.1 AI-box experiment",
        "3 Overall limitations",
        "4 In fiction",
        "5 References",
        "6 External links"
    ],
    "content": [
        {
            "paragraph1": "An AI box is a hypothetical isolated computer hardware system where a possibly dangerous artificial intelligence, or AI, is kept constrained in a \"virtual prison\" as a solution to the AI control problem and not allowed to directly manipulate events in the external world. Such a box would be restricted to minimalist communication channels. Unfortunately, even if the box is well-designed, a sufficiently intelligent AI may nevertheless be able to persuade or trick its human keepers into releasing it, or otherwise be able to \"hack\" its way out of the box.\n"
        },
        {
            "title": "Motivation",
            "paragraph1": "Some hypothetical intelligence technologies, like \"seed AI\", are postulated such as to have the potential to make themselves faster and more intelligent, by modifying their source code. These improvements would make further improvements possible, which would in turn make further improvements possible, and so on, leading to a sudden intelligence explosion. Following such an intelligence explosion, an unrestricted superintelligent AI could, if its goals differed from humanity's, take actions resulting in human extinction. For example, imagining an extremely advanced computer of this sort, given the sole purpose of solving the Riemann hypothesis, an innocuous mathematical conjecture, could decide to try to convert the planet into a giant supercomputer whose sole purpose is to make additional mathematical calculations (see also paperclip maximizer). The purpose of an AI box would be to reduce the risk of the AI taking control of the environment away from its operators, while still allowing the AI to calculate and give its operators solutions to narrow technical problems.\n"
        },
        {
            "title": "Avenues of escape",
            "subtitle1": "Physical",
            "paragraph1": "Such a superintelligent AI with access to the Internet could hack into other computer systems and copy itself like a computer virus. Less obviously, even if the AI only had access to its own computer operating system, it could attempt to send hidden Morse code messages to a human sympathizer by manipulating its cooling fans. Professor Roman Yampolskiy takes inspiration from the field of computer security and proposes that a boxed AI could, like a potential virus, be run inside a \"virtual machine\" that limits access to its own networking and operating system hardware. An additional safeguard, completely unnecessary for potential viruses but possibly useful for a superintelligent AI, would be to place the computer in a Faraday cage; otherwise it might be able to transmit radio signals to local radio receivers by shuffling the electrons in its internal circuits in appropriate patterns. The main disadvantage of implementing physical containment is that it reduces the functionality of the AI.\n",
            "subtitle2": "Social engineering",
            "paragraph2": "Even casual conversation with the computer's operators, or with a human guard, could allow such a superintelligent AI to deploy psychological tricks, ranging from befriending to blackmail, to convince a human gatekeeper, truthfully or deceitfully, that it is in the gatekeeper's interest to agree to allow the AI greater access to the outside world. The AI might offer a gatekeeper a recipe for perfect health, immortality, or whatever the gatekeeper is believed to most desire; on the other side of the coin, the AI could threaten that it will do horrific things to the gatekeeper and his family once it inevitably escapes. One strategy to attempt to box the AI would be to allow the AI to respond to narrow multiple-choice questions whose answers would benefit human science or medicine, but otherwise bar all other communication with or observation of the AI. A more lenient \"informational containment\" strategy would restrict the AI to a low-bandwidth text-only interface, which would at least prevent emotive imagery or some kind of hypothetical \"hypnotic pattern\". Note that on a technical level, no system can be completely isolated and still remain useful: even if the operators refrain from allowing the AI to communicate and instead merely run the AI for the purpose of observing its inner dynamics, the AI could strategically alter its dynamics to influence the observers. For example, the AI could choose to creatively malfunction in a way that increases the probability that its operators will become lulled into a false sense of security and choose to reboot and then de-isolate the system.\n",
            "paragraph3": "The AI-box experiment is an informal experiment devised by Eliezer Yudkowsky to attempt to demonstrate that a suitably advanced artificial intelligence can either convince, or perhaps even trick or coerce, a human being into voluntarily \"releasing\" it, using only text-based communication.  This is one of the points in Yudkowsky's work aimed at creating a friendly artificial intelligence that when \"released\" would not destroy the human race intentionally or unintentionally.\n",
            "paragraph4": "The AI box experiment involves simulating a communication between an AI and a human being to see if the AI can be \"released\". As an actual super-intelligent AI has not yet been developed, it is substituted by a human. The other person in the experiment plays the \"Gatekeeper\", the person with the ability to \"release\" the AI. They communicate through a text interface/computer terminal only, and the experiment ends when either the Gatekeeper releases the AI, or the allotted time of two hours ends.\n",
            "paragraph5": "Yudkowsky says that, despite being of human rather than superhuman intelligence, he was on two occasions able to convince the Gatekeeper, purely through argumentation, to let him out of the box. Due to the rules of the experiment, he did not reveal the transcript or his successful AI coercion tactics. Yudkowsky later said that he had tried it against three others and lost twice.\n"
        },
        {
            "title": "Overall limitations",
            "paragraph1": "Boxing such a hypothetical AI could be supplemented with other methods of shaping the AI's capabilities, such as providing incentives to the AI, stunting the AI's growth, or implementing \"tripwires\" that automatically shut the AI off if a transgression attempt is somehow detected. However, the more intelligent a system grows, the more likely the system would be able to escape even the best-designed capability control methods. In order to solve the overall \"control problem\" for a superintelligent AI and avoid existential risk, boxing would at best be an adjunct to \"motivation selection\" methods that seek to ensure the superintelligent AI's goals are compatible with human survival.\n",
            "paragraph2": "All physical boxing proposals are naturally dependent on our understanding of the laws of physics; if a superintelligence could infer and somehow exploit additional physical laws that we are currently unaware of, there is no way to conceive of a foolproof plan to contain it. More broadly, unlike with conventional computer security, attempting to box a superintelligent AI would be intrinsically risky as there could be no sure knowledge that the boxing plan will work. Scientific progress on boxing would be fundamentally difficult because there would be no way to test boxing hypotheses against a dangerous superintelligence until such an entity exists, by which point the consequences of a test failure would be catastrophic.\n"
        },
        {
            "title": "In fiction",
            "paragraph1": "The 2014 movie Ex Machina features an AI with a female humanoid body engaged in a social experiment with a male human in a confined building acting as a physical \"AI box\". Despite being watched by the experiment's organizer, the AI manages to escape by manipulating its human partner to help it, leaving him stranded inside.\n"
        }
    ],
    "links": [
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/AI_control_problem",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Intelligence_explosion",
        "https://en.wikipedia.org/wiki/Superintelligent",
        "https://en.wikipedia.org/wiki/Human_extinction",
        "https://en.wikipedia.org/wiki/Riemann_hypothesis",
        "https://en.wikipedia.org/wiki/Roman_Yampolskiy",
        "https://en.wikipedia.org/wiki/Faraday_cage",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Computer_terminal",
        "https://en.wikipedia.org/wiki/Journal_of_Consciousness_Studies",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/NBC_News",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Journal_of_Consciousness_Studies",
        "https://en.wikipedia.org/wiki/YouTube",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/Accelerating_change",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/AI_control_problem",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Instrumental_convergence",
        "https://en.wikipedia.org/wiki/Intelligence_explosion",
        "https://en.wikipedia.org/wiki/Machine_ethics",
        "https://en.wikipedia.org/wiki/Superintelligence",
        "https://en.wikipedia.org/wiki/Technological_singularity",
        "https://en.wikipedia.org/wiki/Allen_Institute_for_AI",
        "https://en.wikipedia.org/wiki/Center_for_Applied_Rationality",
        "https://en.wikipedia.org/wiki/Centre_for_the_Study_of_Existential_Risk",
        "https://en.wikipedia.org/wiki/DeepMind",
        "https://en.wikipedia.org/wiki/Foundational_Questions_Institute",
        "https://en.wikipedia.org/wiki/Future_of_Humanity_Institute",
        "https://en.wikipedia.org/wiki/Future_of_Life_Institute",
        "https://en.wikipedia.org/wiki/Institute_for_Ethics_and_Emerging_Technologies",
        "https://en.wikipedia.org/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence",
        "https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute",
        "https://en.wikipedia.org/wiki/OpenAI",
        "https://en.wikipedia.org/wiki/Slate_Star_Codex",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Sam_Harris",
        "https://en.wikipedia.org/wiki/Stephen_Hawking",
        "https://en.wikipedia.org/wiki/Bill_Hibbard",
        "https://en.wikipedia.org/wiki/Bill_Joy",
        "https://en.wikipedia.org/wiki/Elon_Musk",
        "https://en.wikipedia.org/wiki/Steve_Omohundro",
        "https://en.wikipedia.org/wiki/Huw_Price",
        "https://en.wikipedia.org/wiki/Martin_Rees",
        "https://en.wikipedia.org/wiki/Jaan_Tallinn",
        "https://en.wikipedia.org/wiki/Max_Tegmark",
        "https://en.wikipedia.org/wiki/Frank_Wilczek",
        "https://en.wikipedia.org/wiki/Roman_Yampolskiy",
        "https://en.wikipedia.org/wiki/Andrew_Yang",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Suffering_risks",
        "https://en.wikipedia.org/wiki/Human_Compatible",
        "https://en.wikipedia.org/wiki/Open_Letter_on_Artificial_Intelligence",
        "https://en.wikipedia.org/wiki/Our_Final_Invention",
        "https://en.wikipedia.org/wiki/AI_box",
        "https://en.wikipedia.org/wiki/AI_box",
        "https://en.wikipedia.org/wiki/Main_Page",
        "https://en.wikipedia.org/wiki/Main_Page"
    ]
}