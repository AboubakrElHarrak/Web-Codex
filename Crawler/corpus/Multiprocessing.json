{
    "url": "https://en.wikipedia.org/wiki/Multiprocessing",
    "title": "Multiprocessing",
    "table_of_contents": [
        "1 Pre-history",
        "2 Key topics",
        "2.1 Processor symmetry",
        "2.1.1 Master/slave multiprocessor system",
        "2.2 Instruction and data streams",
        "2.3 Processor coupling",
        "2.3.1 Tightly coupled multiprocessor system",
        "2.3.2 Loosely coupled multiprocessor system",
        "3 See also",
        "4 References"
    ],
    "content": [
        {
            "paragraph1": "Multiprocessing is the use of two or more central processing units (CPUs) within a single computer system. The term also refers to the ability of a system to support more than one processor or the ability to allocate tasks between them. There are many variations on this basic theme, and the definition of multiprocessing can vary with context, mostly as a function of how CPUs are defined (multiple cores on one die, multiple dies in one package, multiple packages in one system unit, etc.).\n",
            "paragraph2": "According to some on-line dictionaries, a multiprocessor is a computer system having two or more processing units (multiple processors) each sharing main memory and peripherals, in order to simultaneously process programs. A 2009 textbook defined multiprocessor system similarly, but noting that the processors may share \"some or all of the systemâ€™s memory and I/O facilities\"; it also gave tightly coupled system as a synonymous term.\n",
            "paragraph3": "At the operating system level, multiprocessing is sometimes used to refer to the execution of multiple concurrent processes in a system, with each process running on a separate CPU or core, as opposed to a single process at any one instant. When used with this definition, multiprocessing is sometimes contrasted with multitasking, which may use just a single processor but switch it in time slices between tasks (i.e. a time-sharing system). Multiprocessing however means true parallel execution of multiple processes using more than one processor. Multiprocessing doesn't necessarily mean that a single process or task uses more than one processor simultaneously; the term parallel processing is generally used to denote that scenario. Other authors prefer to refer to the operating system techniques as multiprogramming and reserve the term multiprocessing for the hardware aspect of having more than one processor. The remainder of this article discusses multiprocessing only in this hardware sense.\n",
            "paragraph4": "In Flynn's taxonomy, multiprocessors as defined above are MIMD machines. As the term \"multiprocessor\" normally refers to tightly coupled systems in which all processors share memory, multiprocessors are not the entire class of MIMD machines, which also contains message passing multicomputer systems.\n"
        },
        {
            "title": "Pre-history",
            "paragraph1": "Possibly the first expression of the idea of multiprocessing was written by Luigi Federico Menabrea in 1842, about Charles Babbage's analytical engine (as translated by Ada Lovelace): \"the machine can be brought into play so as to give several results at the same time, which will greatly abridge the whole amount of the processes.\"\n"
        },
        {
            "title": "Key topics",
            "subtitle1": "Processor symmetry",
            "paragraph1": "In a multiprocessing system, all CPUs may be equal, or some may be reserved for special purposes.  A combination of hardware and operating system software design considerations determine the symmetry (or lack thereof) in a given system.  For example, hardware or software considerations may require that only one particular CPU respond to all hardware interrupts, whereas all other work in the system may be distributed equally among CPUs; or execution of kernel-mode code may be restricted to only one particular CPU, whereas user-mode code may be executed in any combination of processors.  Multiprocessing systems are often easier to design if such restrictions are imposed, but they tend to be less efficient than systems in which all CPUs are utilized.\n",
            "paragraph2": "Systems that treat all CPUs equally are called symmetric multiprocessing (SMP) systems.  In systems where all CPUs are not equal, system resources may be divided in a number of ways, including asymmetric multiprocessing (ASMP), non-uniform memory access (NUMA) multiprocessing, and clustered multiprocessing.\n",
            "paragraph3": "In a master/slave multiprocessor system, the master CPU is in control of the computer and the slave CPU(s) performs assigned tasks.  The CPUs can be completely different in terms of speed and architecture.  Some (or all) of the CPUs can have share common bus, each can also have a private bus (for private resources), or they may be isolated except for a common communications pathway.  Likewise, the CPUs can share common RAM and/or have private RAM that the other processor(s) cannot access.  The roles of master and slave can change from one CPU to another.\n",
            "paragraph4": "An early example of a master/slave multiprocessor system is the Tandy/Radio Shack TRS-80 Model 16 desktop computer which came out in February 1982 and ran the multi-user/multi-tasking Xenix operating system, Microsoft's version of UNIX (called TRS-XENIX).  The Model 16 has 3 microprocessors, an 8-bit Zilog Z80 CPU running at 4MHz, a 16-bit Motorola 68000 CPU running at 6MHz and an Intel 8021 in the keyboard.  When the system was booted, the Z-80 was the master and the Xenix boot process initialized the slave 68000, and then transferred control to the 68000, whereupon the CPUs changed roles and the Z-80 became a slave processor that was responsible for all I/O operations including disk, communications, printer and network, as well as the keyboard and integrated monitor, while the operating system and applications ran on the 68000 CPU.  The Z-80 could be used to do other tasks.\n",
            "paragraph5": "The earlier TRS-80 Model II, which was released in 1979, could also be considered a multiprocessor system as it had both a Z-80 CPU and an Intel 8021 microprocessor in the keyboard.  The 8021 made the Model II the first desktop computer system with a separate detachable lightweight keyboard connected with by a single thin flexible wire, and likely the first keyboard to use a dedicated microprocessor, both attributes that would later be copied years later by Apple and IBM.\n",
            "subtitle2": "Instruction and data streams",
            "paragraph6": "In multiprocessing, the processors can be used to execute a single sequence of instructions in multiple contexts (single-instruction, multiple-data or SIMD, often used in vector processing), multiple sequences of instructions in a single context (multiple-instruction, single-data or MISD, used for redundancy in fail-safe systems and sometimes applied to describe pipelined processors or hyper-threading), or multiple sequences of instructions in multiple contexts (multiple-instruction, multiple-data or MIMD).\n",
            "subtitle3": "Processor coupling",
            "paragraph7": "Tightly coupled multiprocessor systems contain multiple CPUs that are connected at the bus level.  These CPUs may have access to a central shared memory (SMP or UMA), or may participate in a memory hierarchy with both local and shared memory (SM)(NUMA). The IBM p690 Regatta is an example of a high end SMP system. Intel Xeon processors dominated the multiprocessor market for business PCs and were the only major x86 option until the release of AMD's Opteron range of processors in 2004. Both ranges of processors had their own onboard cache but provided access to shared memory; the Xeon processors via a common pipe and the Opteron processors via independent pathways to the system RAM.\n",
            "paragraph8": "Chip multiprocessors, also known as multi-core computing, involves more than one processor placed on a single chip and can be thought of the most extreme form of tightly coupled multiprocessing. Mainframe systems with multiple processors are often tightly coupled.\n",
            "paragraph9": "Loosely coupled multiprocessor systems (often referred to as clusters) are based on multiple standalone single or dual processor commodity computers interconnected via a high speed communication system (Gigabit Ethernet is common). A Linux Beowulf cluster is an example of a loosely coupled system.\n",
            "paragraph10": "Tightly coupled systems perform better and are physically smaller than loosely coupled systems, but have historically required greater initial investments and may depreciate rapidly; nodes in a loosely coupled system are usually inexpensive commodity computers and can be recycled as independent machines upon retirement from the cluster.\n",
            "paragraph11": "Power consumption is also a consideration. Tightly coupled systems tend to be much more energy efficient than clusters. This is because considerable economy can be realized by designing components to work together from the beginning in tightly coupled systems, whereas loosely coupled systems use components that were not necessarily intended specifically for use in such systems.\n",
            "paragraph12": "Loosely coupled systems have the ability to run different operating systems or OS versions on different systems.\n"
        }
    ],
    "links": [
        "https://en.wikipedia.org/wiki/CPU",
        "https://en.wikipedia.org/wiki/Computer_system",
        "https://en.wikipedia.org/wiki/Chip_carrier",
        "https://en.wikipedia.org/wiki/System_unit",
        "https://en.wikipedia.org/wiki/Central_processing_unit",
        "https://en.wikipedia.org/wiki/Main_memory",
        "https://en.wikipedia.org/wiki/Operating_system",
        "https://en.wikipedia.org/wiki/Computer_multitasking",
        "https://en.wikipedia.org/wiki/Parallel_computing",
        "https://en.wikipedia.org/wiki/Multiprogramming",
        "https://en.wikipedia.org/wiki/MIMD",
        "https://en.wikipedia.org/wiki/Message_passing",
        "https://en.wikipedia.org/wiki/Luigi_Federico_Menabrea",
        "https://en.wikipedia.org/wiki/Charles_Babbage",
        "https://en.wikipedia.org/wiki/Analytical_engine",
        "https://en.wikipedia.org/wiki/Ada_Lovelace",
        "https://en.wikipedia.org/wiki/Operating_system",
        "https://en.wikipedia.org/wiki/Symmetric_multiprocessing",
        "https://en.wikipedia.org/wiki/Asymmetric_multiprocessing",
        "https://en.wikipedia.org/wiki/Computer_cluster",
        "https://en.wikipedia.org/wiki/Xenix",
        "https://en.wikipedia.org/wiki/Zilog_Z80",
        "https://en.wikipedia.org/wiki/Motorola_68000",
        "https://en.wikipedia.org/wiki/Intel_8021",
        "https://en.wikipedia.org/wiki/SIMD",
        "https://en.wikipedia.org/wiki/Vector_processing",
        "https://en.wikipedia.org/wiki/MISD",
        "https://en.wikipedia.org/wiki/MIMD",
        "https://en.wikipedia.org/wiki/Uniform_Memory_Access",
        "https://en.wikipedia.org/wiki/IBM_p690",
        "https://en.wikipedia.org/wiki/Intel",
        "https://en.wikipedia.org/wiki/Xeon",
        "https://en.wikipedia.org/wiki/AMD",
        "https://en.wikipedia.org/wiki/Opteron",
        "https://en.wikipedia.org/wiki/Shared_nothing_architecture",
        "https://en.wikipedia.org/wiki/Computer_cluster",
        "https://en.wikipedia.org/wiki/Commodity_computer",
        "https://en.wikipedia.org/wiki/Gigabit_Ethernet",
        "https://en.wikipedia.org/wiki/Beowulf_cluster",
        "https://en.wikipedia.org/wiki/Loose_coupling",
        "https://en.wikipedia.org/wiki/Depreciation",
        "https://en.wikipedia.org/wiki/Multiprocessor_system_architecture",
        "https://en.wikipedia.org/wiki/Symmetric_multiprocessing",
        "https://en.wikipedia.org/wiki/Asymmetric_multiprocessing",
        "https://en.wikipedia.org/wiki/BMDFM",
        "https://en.wikipedia.org/wiki/Software_lockout",
        "https://en.wikipedia.org/wiki/OpenHMPP",
        "https://en.wikipedia.org/wiki/Luigi_Federico_Menabrea",
        "https://en.wikipedia.org/wiki/Parallel_computing",
        "https://en.wikipedia.org/wiki/Distributed_computing",
        "https://en.wikipedia.org/wiki/Parallel_computing",
        "https://en.wikipedia.org/wiki/Massively_parallel",
        "https://en.wikipedia.org/wiki/Cloud_computing",
        "https://en.wikipedia.org/wiki/Supercomputer",
        "https://en.wikipedia.org/wiki/Manycore_processor",
        "https://en.wikipedia.org/wiki/Computer_network",
        "https://en.wikipedia.org/wiki/Systolic_array",
        "https://en.wikipedia.org/wiki/Task_parallelism",
        "https://en.wikipedia.org/wiki/Task_parallelism",
        "https://en.wikipedia.org/wiki/Data_parallelism",
        "https://en.wikipedia.org/wiki/Temporal_multithreading",
        "https://en.wikipedia.org/wiki/Simultaneous_multithreading",
        "https://en.wikipedia.org/wiki/Speculative_multithreading",
        "https://en.wikipedia.org/wiki/Hardware_scout",
        "https://en.wikipedia.org/wiki/Parallel_external_memory",
        "https://en.wikipedia.org/wiki/Analysis_of_parallel_algorithms",
        "https://en.wikipedia.org/wiki/Cost_efficiency",
        "https://en.wikipedia.org/wiki/Parallel_slowdown",
        "https://en.wikipedia.org/wiki/Speedup",
        "https://en.wikipedia.org/wiki/Instruction_window",
        "https://en.wikipedia.org/wiki/Array_data_structure",
        "https://en.wikipedia.org/wiki/Memory_coherence",
        "https://en.wikipedia.org/wiki/Cache_coherence",
        "https://en.wikipedia.org/wiki/Cache_invalidation",
        "https://en.wikipedia.org/wiki/Application_checkpointing",
        "https://en.wikipedia.org/wiki/Computer_programming",
        "https://en.wikipedia.org/wiki/Stream_processing",
        "https://en.wikipedia.org/wiki/Dataflow_programming",
        "https://en.wikipedia.org/wiki/Parallel_programming_model",
        "https://en.wikipedia.org/wiki/Implicit_parallelism",
        "https://en.wikipedia.org/wiki/Explicit_parallelism",
        "https://en.wikipedia.org/wiki/Computer_hardware",
        "https://en.wikipedia.org/wiki/SISD",
        "https://en.wikipedia.org/wiki/SIMD",
        "https://en.wikipedia.org/wiki/MISD",
        "https://en.wikipedia.org/wiki/MIMD",
        "https://en.wikipedia.org/wiki/Dataflow_architecture",
        "https://en.wikipedia.org/wiki/Instruction_pipelining",
        "https://en.wikipedia.org/wiki/Superscalar_processor",
        "https://en.wikipedia.org/wiki/Vector_processor",
        "https://en.wikipedia.org/wiki/Symmetric_multiprocessing",
        "https://en.wikipedia.org/wiki/Asymmetric_multiprocessing",
        "https://en.wikipedia.org/wiki/Semiconductor_memory",
        "https://en.wikipedia.org/wiki/Shared_memory",
        "https://en.wikipedia.org/wiki/Distributed_memory",
        "https://en.wikipedia.org/wiki/Distributed_shared_memory",
        "https://en.wikipedia.org/wiki/Uniform_memory_access",
        "https://en.wikipedia.org/wiki/Massively_parallel",
        "https://en.wikipedia.org/wiki/Computer_cluster",
        "https://en.wikipedia.org/wiki/Grid_computing",
        "https://en.wikipedia.org/wiki/Hardware_acceleration",
        "https://en.wikipedia.org/wiki/Application_programming_interface",
        "https://en.wikipedia.org/wiki/Ateji_PX",
        "https://en.wikipedia.org/wiki/HPX",
        "https://en.wikipedia.org/wiki/Cilk",
        "https://en.wikipedia.org/wiki/Coarray_Fortran",
        "https://en.wikipedia.org/wiki/CUDA",
        "https://en.wikipedia.org/wiki/Global_Arrays",
        "https://en.wikipedia.org/wiki/GPUOpen",
        "https://en.wikipedia.org/wiki/Message_Passing_Interface",
        "https://en.wikipedia.org/wiki/OpenMP",
        "https://en.wikipedia.org/wiki/OpenCL",
        "https://en.wikipedia.org/wiki/OpenHMPP",
        "https://en.wikipedia.org/wiki/OpenACC",
        "https://en.wikipedia.org/wiki/Parallel_Extensions",
        "https://en.wikipedia.org/wiki/Parallel_Virtual_Machine",
        "https://en.wikipedia.org/wiki/POSIX_Threads",
        "https://en.wikipedia.org/wiki/RaftLib",
        "https://en.wikipedia.org/wiki/ROCm",
        "https://en.wikipedia.org/wiki/Unified_Parallel_C",
        "https://en.wikipedia.org/wiki/Threading_Building_Blocks",
        "https://en.wikipedia.org/wiki/Automatic_parallelization",
        "https://en.wikipedia.org/wiki/Deadlock",
        "https://en.wikipedia.org/wiki/Deterministic_algorithm",
        "https://en.wikipedia.org/wiki/Embarrassingly_parallel",
        "https://en.wikipedia.org/wiki/Parallel_slowdown",
        "https://en.wikipedia.org/wiki/Race_condition",
        "https://en.wikipedia.org/wiki/Software_lockout",
        "https://en.wikipedia.org/wiki/Scalability",
        "https://en.wikipedia.org/wiki/Multiprocessing",
        "https://en.wikipedia.org/wiki/Multiprocessing",
        "https://en.wikipedia.org/wiki/Main_Page",
        "https://en.wikipedia.org/wiki/Main_Page"
    ]
}